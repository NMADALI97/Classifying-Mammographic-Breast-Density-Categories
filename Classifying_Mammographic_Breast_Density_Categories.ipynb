{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying-Mammographic-Breast-Density-Categories.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HscEHOPV1aJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile, os\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "zip_id = '1upOcakmV4Ym2sv-5K3MY88rA_QNzTXJV'\n",
        "print (\"Downloading zip file\")\n",
        "myzip = drive.CreateFile({'id': zip_id})\n",
        "myzip.GetContentFile('model.zip')\n",
        "print (\"Uncompressing zip file\")\n",
        "zip_ref = zipfile.ZipFile('model.zip', 'r')\n",
        "zip_ref.extractall(\"breast_density_dataset\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KquFZ80k3He5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "valid = pd.read_csv(\"breast_density_dataset/train.csv\",nrows=2000)\n",
        "train = pd.read_csv(\"breast_density_dataset/train.csv\", skiprows=2000)\n",
        "train.columns = valid.columns\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "for col in valid.columns[2:-1]:\n",
        "  valid[col]=le.fit_transform(valid[col])\n",
        "  train[col]=le.fit_transform(train[col])\n",
        "\n",
        "train =train.sample(frac=1).reset_index(drop=True)\n",
        "valid =valid.sample(frac=1).reset_index(drop=True)\n",
        "train.to_csv(\"train.csv\",index=False)\n",
        "valid.to_csv(\"valid.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iELMj1M1dd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import h5py\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def read_image_and_breast_mask(h5_filename):\n",
        "    # 'r' mode is very important ! \n",
        "    with h5py.File(h5_filename, \"r\") as h5_file:\n",
        "        image = h5_file[\"image\"][:]\n",
        "        breast_mask = h5_file[\"mask\"][:]\n",
        "\n",
        "    return image, breast_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTpbWFFT2zbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(Train_df,batch_size,steps):\n",
        " idx=1\n",
        " while True: \n",
        "  yield load_data(Train_df,idx-1,batch_size)## Yields data\n",
        "  if idx<steps:\n",
        "    idx+=1\n",
        "  else:\n",
        "    idx=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f6TZR_z2X6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = \"breast_density_dataset/train/\"\n",
        "TEST_DIR = \"breast_density_dataset/test/\"\n",
        "\n",
        "def load_data(Train_df,idx,batch_size):\n",
        " df = pd.read_csv(Train_df, skiprows=idx*batch_size,nrows=batch_size)\n",
        " x = []\n",
        " z = []\n",
        " y = [] \n",
        " for  row  in df.values:\n",
        "  image_uid=row[0]\n",
        "  image_file = os.path.join(TRAIN_DIR, image_uid + \".h5\")\n",
        "  X, breast_mask = read_image_and_breast_mask(image_file)\n",
        "\n",
        "  image = (X - X.min()) / (X.max() - X.min())\n",
        "  #image =(image*255.).astype('uint8')\n",
        "  \n",
        " # tmp=np.zeros(2)\n",
        " # tmp[row[-2]-1]=1\n",
        "  #latent=np.array(functors([image.reshape(1,288, 208, 1), 1.])).reshape(36, 26, 128)\n",
        "  #z.append(row[2:-1])\n",
        "  x.append(image) \n",
        "  y.append(row[2:] )\n",
        "\n",
        " return (np.array(x).reshape(-1,288,208,1), np.array(y).reshape(-1,5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujU7_58f2iLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "batch_size=32\n",
        "steps_per_epoch=np.ceil(7960/batch_size)\n",
        "validation_steps=np.ceil(2000/batch_size)\n",
        "\n",
        "my_training_batch_generator = batch_generator('train.csv', 32,steps_per_epoch)\n",
        "my_validation_batch_generator = batch_generator('valid.csv', 32,validation_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF4PM2Up2kH6",
        "colab_type": "code",
        "outputId": "8292f9b2-a38e-4d3d-9431-7ca5f7454f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D,Flatten,Reshape,Convolution2D,Cropping2D\n",
        "from keras.models import Model\n",
        "from skimage.transform import resize\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "\n",
        "input_img = Input(shape=(288,208,1))  # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = Conv2D(32, (5, 5), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(64, (5, 5), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(64, (5, 5), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(128, (5, 5), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "\n",
        "\n",
        "x=Flatten()(x)\n",
        "x = Dense(1024)(x)\n",
        "\n",
        "x = Dense(512)(x)\n",
        "\n",
        "x = Dense(256)(x)\n",
        "\n",
        "output = Dense(5,activation='relu')(x)\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "model = Model(input_img, [output])\n",
        "model.compile(optimizer='adam', loss=['mse'])\n",
        "model.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 288, 208, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 288, 208, 32)      832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 144, 104, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 144, 104, 64)      51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 72, 52, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 72, 52, 128)       204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 36, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 18, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 18, 13, 64)        204864    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_50 (MaxPooling (None, 9, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 9, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_51 (MaxPooling (None, 5, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 2560)              0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 1024)              2622464   \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,946,693\n",
            "Trainable params: 3,946,693\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9eJzq12luU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(patience=10, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint('model-s.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR1BAHbP2nWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "12f54a32-a43c-4a19-d5e3-8bc500cd1a3e"
      },
      "source": [
        "results = model.fit_generator(my_training_batch_generator,epochs=30,steps_per_epoch=steps_per_epoch,verbose=1, validation_data=my_validation_batch_generator,validation_steps=validation_steps, callbacks=callbacks)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "249/249 [==============================] - 32s 130ms/step - loss: 0.9357 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69367, saving model to model-s.h5\n",
            "Epoch 2/30\n",
            "249/249 [==============================] - 28s 114ms/step - loss: 0.6732 - val_loss: 0.5437\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69367 to 0.54365, saving model to model-s.h5\n",
            "Epoch 3/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.5498 - val_loss: 0.5263\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.54365 to 0.52626, saving model to model-s.h5\n",
            "Epoch 4/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.5382 - val_loss: 0.5194\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.52626 to 0.51940, saving model to model-s.h5\n",
            "Epoch 5/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.5297 - val_loss: 0.5113\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.51940 to 0.51130, saving model to model-s.h5\n",
            "Epoch 6/30\n",
            "249/249 [==============================] - 29s 118ms/step - loss: 0.5234 - val_loss: 0.5034\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.51130 to 0.50340, saving model to model-s.h5\n",
            "Epoch 7/30\n",
            "249/249 [==============================] - 30s 121ms/step - loss: 0.5192 - val_loss: 0.5034\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.50340 to 0.50338, saving model to model-s.h5\n",
            "Epoch 8/30\n",
            "249/249 [==============================] - 29s 117ms/step - loss: 0.5146 - val_loss: 0.5048\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.50338\n",
            "Epoch 9/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.5105 - val_loss: 0.4978\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.50338 to 0.49783, saving model to model-s.h5\n",
            "Epoch 10/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.5058 - val_loss: 0.4965\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.49783 to 0.49648, saving model to model-s.h5\n",
            "Epoch 11/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.1535 - val_loss: 0.1249\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.49648 to 0.12494, saving model to model-s.h5\n",
            "Epoch 12/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.1047 - val_loss: 0.1107\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.12494 to 0.11072, saving model to model-s.h5\n",
            "Epoch 13/30\n",
            "249/249 [==============================] - 30s 119ms/step - loss: 0.0954 - val_loss: 0.1084\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.11072 to 0.10839, saving model to model-s.h5\n",
            "Epoch 14/30\n",
            "249/249 [==============================] - 30s 120ms/step - loss: 0.0891 - val_loss: 0.1086\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.10839\n",
            "Epoch 15/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.0842 - val_loss: 0.1057\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.10839 to 0.10568, saving model to model-s.h5\n",
            "Epoch 16/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0786 - val_loss: 0.1081\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.10568\n",
            "Epoch 17/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.0747 - val_loss: 0.1083\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.10568\n",
            "Epoch 18/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.0721 - val_loss: 0.1207\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.10568\n",
            "Epoch 19/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0595 - val_loss: 0.0953\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.10568 to 0.09530, saving model to model-s.h5\n",
            "Epoch 20/30\n",
            "249/249 [==============================] - 30s 121ms/step - loss: 0.0534 - val_loss: 0.0953\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.09530\n",
            "Epoch 21/30\n",
            "249/249 [==============================] - 29s 118ms/step - loss: 0.0505 - val_loss: 0.0955\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.09530\n",
            "Epoch 22/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0482 - val_loss: 0.0960\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.09530\n",
            "Epoch 23/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.0449 - val_loss: 0.0927\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.09530 to 0.09273, saving model to model-s.h5\n",
            "Epoch 24/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0437 - val_loss: 0.0927\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.09273 to 0.09271, saving model to model-s.h5\n",
            "Epoch 25/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0433 - val_loss: 0.0928\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.09271\n",
            "Epoch 26/30\n",
            "249/249 [==============================] - 30s 119ms/step - loss: 0.0429 - val_loss: 0.0929\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.09271\n",
            "Epoch 27/30\n",
            "249/249 [==============================] - 30s 121ms/step - loss: 0.0426 - val_loss: 0.0929\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.09271\n",
            "Epoch 28/30\n",
            "249/249 [==============================] - 29s 117ms/step - loss: 0.0423 - val_loss: 0.0930\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.09271\n",
            "Epoch 29/30\n",
            "249/249 [==============================] - 29s 115ms/step - loss: 0.0420 - val_loss: 0.0930\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.09271\n",
            "Epoch 30/30\n",
            "249/249 [==============================] - 29s 116ms/step - loss: 0.0417 - val_loss: 0.0931\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.09271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPuHtC68X78O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e369e4db-8cdb-4acd-96d2-2473d12488fc"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "s=0\n",
        "for  row  in df.values:\n",
        "  image_uid=row[0]\n",
        "  image_file = os.path.join(TRAIN_DIR, image_uid + \".h5\")\n",
        "  X, breast_mask = read_image_and_breast_mask(image_file)\n",
        "  image = (X - X.min()) / (X.max() - X.min())\n",
        "  tmp=model.predict(image.reshape(1,288, 208, 1))\n",
        "  p=np.abs(np.round(tmp.reshape(-1)))\n",
        "  if p[-1]==row[-1]:\n",
        "    s+=1\n",
        "print(\"precision :\"+str(s/len(df)))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision :0.821608040201005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M9Tlu71YlpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(\"breast_density_dataset/train.csv\")\n",
        "y_pred=[]\n",
        "for  row  in test.values:\n",
        "  image_uid=row[0]\n",
        "  image_file = os.path.join(TRAIN_DIR, image_uid + \".h5\")\n",
        "  X, breast_mask = read_image_and_breast_mask(image_file)\n",
        "  image = (X - X.min()) / (X.max() - X.min())\n",
        "  tmp=model.predict(image.reshape(1,288, 208, 1))\n",
        "  p=np.abs(np.round(tmp.reshape(-1)))\n",
        "  y_pred.append(int(p[-1]))\n",
        "\n",
        "test[\"breast_density_score\"]=y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMBbsw7KZOED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "bb7bc129-5c7a-4eae-f01c-e37ee27d1782"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_uid</th>\n",
              "      <th>study_uid</th>\n",
              "      <th>source</th>\n",
              "      <th>manufacturer</th>\n",
              "      <th>view</th>\n",
              "      <th>laterality</th>\n",
              "      <th>breast_density_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pJKpQYcIUh</td>\n",
              "      <td>zHcZzUxPOaEdmbhUSHId</td>\n",
              "      <td>BANI</td>\n",
              "      <td>A</td>\n",
              "      <td>CC</td>\n",
              "      <td>L</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>iemeEWCujY</td>\n",
              "      <td>zHcZzUxPOaEdmbhUSHId</td>\n",
              "      <td>BANI</td>\n",
              "      <td>A</td>\n",
              "      <td>MLO</td>\n",
              "      <td>L</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>YmTbYwFFVD</td>\n",
              "      <td>zHcZzUxPOaEdmbhUSHId</td>\n",
              "      <td>BANI</td>\n",
              "      <td>A</td>\n",
              "      <td>CC</td>\n",
              "      <td>R</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WwZyDUkXle</td>\n",
              "      <td>zHcZzUxPOaEdmbhUSHId</td>\n",
              "      <td>BANI</td>\n",
              "      <td>A</td>\n",
              "      <td>MLO</td>\n",
              "      <td>R</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mOFgddQiUt</td>\n",
              "      <td>ljhFclzlCrLgXrDLkIeS</td>\n",
              "      <td>BANI</td>\n",
              "      <td>C</td>\n",
              "      <td>CC</td>\n",
              "      <td>L</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    image_uid             study_uid  ... laterality breast_density_score\n",
              "0  pJKpQYcIUh  zHcZzUxPOaEdmbhUSHId  ...          L                    2\n",
              "1  iemeEWCujY  zHcZzUxPOaEdmbhUSHId  ...          L                    3\n",
              "2  YmTbYwFFVD  zHcZzUxPOaEdmbhUSHId  ...          R                    2\n",
              "3  WwZyDUkXle  zHcZzUxPOaEdmbhUSHId  ...          R                    2\n",
              "4  mOFgddQiUt  ljhFclzlCrLgXrDLkIeS  ...          L                    1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn82DzVgZ9ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv(\"submissions.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}